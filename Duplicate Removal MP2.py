import os
import shutil
import random
import hashlib
from multiprocessing import Pool, cpu_count
import cv2
import imagehash
from PIL import Image

# âœ… ê²½ë¡œ ì„¤ì •
BASE_FOLDER = "raw_data/ì¤‘ë³µì œê±°Image/"
DUPLICATE_FOLDER = os.path.join(BASE_FOLDER, "ì¤‘ë³µëœì´ë¯¸ì§€")
TRAIN_FOLDER = "data/train"
VAL_FOLDER = "data/val"

# âœ… ì¹´í…Œê³ ë¦¬ í´ë” ë¦¬ìŠ¤íŠ¸
def get_categories(base_folder):
    return [f for f in os.listdir(base_folder) if os.path.isdir(os.path.join(base_folder, f)) and f != "ì¤‘ë³µëœì´ë¯¸ì§€"]

# âœ… ì¹´í…Œê³ ë¦¬ ìˆ˜ì— ë”°ë¥¸ ìƒ˜í”Œ ê°œìˆ˜ ì„¤ì •
def get_sample_count(category_count):
    if category_count <= 5:
        return 300
    elif category_count <= 10:
        return 500
    elif category_count <= 20:
        return 700
    else:
        return 1000

# âœ… í´ë” ì¤€ë¹„
def prepare_dirs(categories):
    os.makedirs(DUPLICATE_FOLDER, exist_ok=True)
    for split_dir in [TRAIN_FOLDER, VAL_FOLDER]:
        os.makedirs(split_dir, exist_ok=True)
        for category in categories:
            os.makedirs(os.path.join(split_dir, category), exist_ok=True)

# âœ… í•´ì‹œ ê¸°ë°˜ ì¤‘ë³µ ì œê±° í•¨ìˆ˜ (MD5 + pHash)
def remove_duplicates(category):
    folder_path = os.path.join(BASE_FOLDER, category)
    duplicate_path = os.path.join(DUPLICATE_FOLDER, category)
    os.makedirs(duplicate_path, exist_ok=True)

    hash_dict = {}
    phash_dict = {}
    images = [f for f in os.listdir(folder_path) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]

    for img in images:
        img_path = os.path.join(folder_path, img)

        # MD5 í•´ì‹œ ì²´í¬
        with open(img_path, 'rb') as f:
            img_hash = hashlib.md5(f.read()).hexdigest()

        # ì™„ì „ ë™ì¼í•œ ì´ë¯¸ì§€ ì œê±°
        if img_hash in hash_dict:
            print(f"ğŸ›‘ [MD5 ì¤‘ë³µ ì œê±°] {img} â†” {hash_dict[img_hash]}")
            shutil.move(img_path, os.path.join(duplicate_path, img))
            continue

        # Perceptual Hash (pHash) ì²´í¬
        img_phash = imagehash.phash(Image.open(img_path))
        found_duplicate = False

        for other_img, other_phash in phash_dict.items():
            if img_phash - other_phash <= 3:  # í—ˆìš© ì˜¤ì°¨ (5 ì´í•˜ë¡œ ì„¤ì •)
                print(f"ğŸ›‘ [pHash ì¤‘ë³µ ì œê±°] {img} â†” {other_img}")
                shutil.move(img_path, os.path.join(duplicate_path, img))
                found_duplicate = True
                break

        if not found_duplicate:
            hash_dict[img_hash] = img
            phash_dict[img] = img_phash

    print(f"âœ… [{category}] í´ë”ì—ì„œ ì¤‘ë³µ ì œê±° ì™„ë£Œ! ë‚¨ì€ ì´ë¯¸ì§€: {len(phash_dict)}ê°œ")

# âœ… train/val ë¶„í•  ë° ë³µì‚¬
def split_data(categories):
    sample_per_category = get_sample_count(len(categories))
    train_per_category = int(sample_per_category * 0.7)
    val_per_category = sample_per_category - train_per_category

    for category in categories:
        src_dir = os.path.join(BASE_FOLDER, category)
        images = [f for f in os.listdir(src_dir) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]
        random.shuffle(images)

        selected_images = images[:sample_per_category] if len(images) >= sample_per_category else images
        train_images = selected_images[:train_per_category]
        val_images = selected_images[train_per_category:]

        for img in train_images:
            shutil.copy2(os.path.join(src_dir, img), os.path.join(TRAIN_FOLDER, category, img))
        for img in val_images:
            shutil.copy2(os.path.join(src_dir, img), os.path.join(VAL_FOLDER, category, img))

        print(f"âœ… {category}: Train({len(train_images)}ê°œ) / Val({len(val_images)}ê°œ) ë¶„í•  ì™„ë£Œ!")

if __name__ == '__main__':
    categories = get_categories(BASE_FOLDER)
    prepare_dirs(categories)

    # âœ… ë©€í‹°í”„ë¡œì„¸ì‹±ìœ¼ë¡œ ì¹´í…Œê³ ë¦¬ë³„ ì¤‘ë³µ ì œê±°
    with Pool(processes=cpu_count()) as pool:
        pool.map(remove_duplicates, categories)

    # âœ… ë¶„í•  ì‘ì—…
    split_data(categories)
    print("\nğŸš€ ì¤‘ë³µ ì œê±° ë° Train/Val ë¶„í•  ì™„ë£Œ!")
